#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —á–∞—Å—Ç–æ—Ç–∏ —Å–ª—ñ–≤ —É —Ç–µ–∫—Å—Ç—ñ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º MapReduce —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤.
–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ç–µ–∫—Å—Ç –∑ URL, –∞–Ω–∞–ª—ñ–∑—É—î —á–∞—Å—Ç–æ—Ç—É —Å–ª—ñ–≤ —Ç–∞ –±—É–¥—É—î –≥—Ä–∞—Ñ—ñ–∫ —Ç–æ–ø-—Å–ª—ñ–≤.
"""

import requests
import string
import re
from collections import defaultdict, Counter
from concurrent.futures import ThreadPoolExecutor
import matplotlib.pyplot as plt
from typing import List, Tuple, Dict, Callable, Any
import argparse
import logging

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MapReduceWordAnalyzer:
    """–ö–ª–∞—Å –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —á–∞—Å—Ç–æ—Ç–∏ —Å–ª—ñ–≤ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º MapReduce"""
    
    def __init__(self, num_workers: int = 4):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä–∞
        
        Args:
            num_workers: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Ç–æ–∫—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏
        """
        self.num_workers = num_workers
        self.stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after', 'above', 'below',
            'as', 'is', 'was', 'are', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does',
            'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'shall', 'this',
            'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'them', 'their',
            'what', 'which', 'who', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each',
            'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same',
            'so', 'than', 'too', 'very', 's', 't', 'just', 'don', 'now', 'if', 'then', 'once'
        }
    
    def fetch_text_from_url(self, url: str) -> str:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ç–µ–∫—Å—Ç –∑ URL
        
        Args:
            url: URL –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É
            
        Returns:
            –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π —Ç–µ–∫—Å—Ç
        """
        logger.info(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ URL: {url}")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            text = response.text
            logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª—ñ–≤")
            return text
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É: {e}")
            raise
    
    def preprocess_text(self, text: str) -> str:
        """
        –ü–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞ —Ç–µ–∫—Å—Ç—É
        
        Args:
            text: –í–∏—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –û—á–∏—â–µ–Ω–∏–π —Ç–µ–∫—Å—Ç
        """
        logger.info("üßπ –ü–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞ —Ç–µ–∫—Å—Ç—É...")
        
        # –í–∏–¥–∞–ª—è—î–º–æ HTML —Ç–µ–≥–∏ —è–∫—â–æ —î
        text = re.sub(r'<[^>]+>', '', text)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —É –Ω–∏–∂–Ω—ñ–π —Ä–µ–≥—ñ—Å—Ç—Ä
        text = text.lower()
        
        # –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏ —Ç–∞ –ø–µ—Ä–µ–Ω–æ—Å–∏ —Ä—è–¥–∫—ñ–≤
        text = re.sub(r'\s+', ' ', text)
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Ü–∏—Ñ—Ä–∏ —Ç–∞ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ —Å–∏–º–≤–æ–ª–∏ (–∑–∞–ª–∏—à–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –±—É–∫–≤–∏ —Ç–∞ –ø—Ä–æ–±—ñ–ª–∏)
        text = re.sub(r'[^a-z\s]', '', text)
        
        logger.info("‚úÖ –¢–µ–∫—Å—Ç –æ—á–∏—â–µ–Ω–æ")
        return text.strip()
    
    def map_function(self, text_chunk: str) -> List[Tuple[str, int]]:
        """
        Map —Ñ—É–Ω–∫—Ü—ñ—è: —Ä–æ–∑–±–∏–≤–∞—î —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞ —Ç–∞ —Ä–∞—Ö—É—î —ó—Ö
        
        Args:
            text_chunk: –ß–∞—Å—Ç–∏–Ω–∞ —Ç–µ–∫—Å—Ç—É –¥–ª—è –æ–±—Ä–æ–±–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø–∞—Ä (—Å–ª–æ–≤–æ, –∫—ñ–ª—å–∫—ñ—Å—Ç—å)
        """
        words = text_chunk.split()
        word_count = []
        
        for word in words:
            word = word.strip()
            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –∫–æ—Ä–æ—Ç–∫—ñ —Å–ª–æ–≤–∞ —Ç–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            if len(word) > 2 and word not in self.stop_words:
                word_count.append((word, 1))
        
        return word_count
    
    def reduce_function(self, mapped_data: List[List[Tuple[str, int]]]) -> Dict[str, int]:
        """
        Reduce —Ñ—É–Ω–∫—Ü—ñ—è: –∞–≥—Ä–µ–≥—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É
        
        Args:
            mapped_data: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ map –æ–ø–µ—Ä–∞—Ü—ñ–π
            
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ —á–∞—Å—Ç–æ—Ç–æ—é —Å–ª—ñ–≤
        """
        word_count = defaultdict(int)
        
        for word_list in mapped_data:
            for word, count in word_list:
                word_count[word] += count
        
        return dict(word_count)
    
    def split_text_into_chunks(self, text: str, num_chunks: int) -> List[str]:
        """
        –†–æ–∑–¥—ñ–ª—è—î —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ –¥–ª—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è
            num_chunks: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —á–∞—Å—Ç–∏–Ω
            
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞—Å—Ç–∏–Ω —Ç–µ–∫—Å—Ç—É
        """
        words = text.split()
        chunk_size = len(words) // num_chunks
        
        chunks = []
        for i in range(num_chunks):
            start = i * chunk_size
            
            if i == num_chunks - 1:  # –û—Å—Ç–∞–Ω–Ω—è —á–∞—Å—Ç–∏–Ω–∞
                end = len(words)
            else:
                end = (i + 1) * chunk_size
            
            chunk = ' '.join(words[start:end])
            chunks.append(chunk)
        
        return chunks
    
    def mapreduce_word_count(self, text: str) -> Dict[str, int]:
        """
        –í–∏–∫–æ–Ω—É—î MapReduce –∞–Ω–∞–ª—ñ–∑ —á–∞—Å—Ç–æ—Ç–∏ —Å–ª—ñ–≤
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
            
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ —á–∞—Å—Ç–æ—Ç–æ—é —Å–ª—ñ–≤
        """
        logger.info("üîÑ –ü–æ—á–∞—Ç–æ–∫ MapReduce –∞–Ω–∞–ª—ñ–∑—É...")
        
        # –ü–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞ —Ç–µ–∫—Å—Ç—É
        text = self.preprocess_text(text)
        
        # –†–æ–∑–¥—ñ–ª—è—î–º–æ —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏
        text_chunks = self.split_text_into_chunks(text, self.num_workers)
        logger.info(f"üìù –¢–µ–∫—Å—Ç —Ä–æ–∑–¥—ñ–ª–µ–Ω–æ –Ω–∞ {len(text_chunks)} —á–∞—Å—Ç–∏–Ω")
        
        # Map —Ñ–∞–∑–∞: –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ —á–∞—Å—Ç–∏–Ω —Ç–µ–∫—Å—Ç—É
        logger.info("üó∫Ô∏è  Map —Ñ–∞–∑–∞: –æ–±—Ä–æ–±–∫–∞ —á–∞—Å—Ç–∏–Ω —Ç–µ–∫—Å—Ç—É...")
        with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            mapped_results = list(executor.map(self.map_function, text_chunks))
        
        logger.info(f"‚úÖ Map —Ñ–∞–∑–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –æ–±—Ä–æ–±–ª–µ–Ω–æ {len(mapped_results)} —á–∞—Å—Ç–∏–Ω")
        
        # Reduce —Ñ–∞–∑–∞: –∞–≥—Ä–µ–≥–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        logger.info("üîç Reduce —Ñ–∞–∑–∞: –∞–≥—Ä–µ–≥–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤...")
        word_frequencies = self.reduce_function(mapped_results)
        
        logger.info(f"‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ó–Ω–∞–π–¥–µ–Ω–æ {len(word_frequencies)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å–ª—ñ–≤")
        return word_frequencies
    
    def get_top_words(self, word_frequencies: Dict[str, int], top_n: int = 10) -> List[Tuple[str, int]]:
        """
        –û—Ç—Ä–∏–º—É—î —Ç–æ–ø-N –Ω–∞–π—á–∞—Å—Ç—ñ—à–∏—Ö —Å–ª—ñ–≤
        
        Args:
            word_frequencies: –°–ª–æ–≤–Ω–∏–∫ –∑ —á–∞—Å—Ç–æ—Ç–æ—é —Å–ª—ñ–≤
            top_n: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–ø-—Å–ª—ñ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–ø-—Å–ª—ñ–≤ –∑ —á–∞—Å—Ç–æ—Ç–æ—é
        """
        counter = Counter(word_frequencies)
        top_words = counter.most_common(top_n)
        
        logger.info(f"üèÜ –¢–æ–ø-{top_n} –Ω–∞–π—á–∞—Å—Ç—ñ—à–∏—Ö —Å–ª—ñ–≤:")
        for i, (word, count) in enumerate(top_words, 1):
            logger.info(f"   {i}. '{word}': {count} —Ä–∞–∑—ñ–≤")
        
        return top_words
    
    def visualize_top_words(self, top_words: List[Tuple[str, int]], 
                           title: str = "–¢–æ–ø —Å–ª–æ–≤–∞ –∑–∞ —á–∞—Å—Ç–æ—Ç–æ—é –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è") -> None:
        """
        –í—ñ–∑—É–∞–ª—ñ–∑—É—î —Ç–æ–ø-—Å–ª–æ–≤–∞ –Ω–∞ –≥—Ä–∞—Ñ—ñ–∫—É
        
        Args:
            top_words: –°–ø–∏—Å–æ–∫ —Ç–æ–ø-—Å–ª—ñ–≤ –∑ —á–∞—Å—Ç–æ—Ç–æ—é
            title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä–∞—Ñ—ñ–∫—É
        """
        logger.info("üìä –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó...")
        
        if not top_words:
            logger.warning("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó")
            return
        
        words, frequencies = zip(*top_words)
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—É
        plt.figure(figsize=(12, 8))
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Å—Ç–æ–≤–ø—á–∏–∫–æ–≤—É –¥—ñ–∞–≥—Ä–∞–º—É
        bars = plt.bar(words, frequencies, color='skyblue', edgecolor='navy', alpha=0.7)
        
        # –î–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞ —Å—Ç–æ–≤–ø—á–∏–∫–∏
        for bar, freq in zip(bars, frequencies):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{freq}', ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –æ—Å–µ–π —Ç–∞ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤
        plt.title(title, fontsize=16, fontweight='bold', pad=20)
        plt.xlabel('–°–ª–æ–≤–∞', fontsize=12, fontweight='bold')
        plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è', fontsize=12, fontweight='bold')
        
        # –ü–æ–≤–æ—Ä–æ—Ç –ø—ñ–¥–ø–∏—Å—ñ–≤ –Ω–∞ –æ—Å—ñ X –¥–ª—è –∫—Ä–∞—â–æ—ó —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—ñ
        plt.xticks(rotation=45, ha='right')
        
        # –î–æ–¥–∞–≤–∞–Ω–Ω—è —Å—ñ—Ç–∫–∏
        plt.grid(axis='y', alpha=0.3, linestyle='--')
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—ñ–≤
        plt.tight_layout()
        
        # –ü–æ–∫–∞–∑—É—î–º–æ –≥—Ä–∞—Ñ—ñ–∫
        plt.show()
        
        logger.info("‚úÖ –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Å—Ç–≤–æ—Ä–µ–Ω–∞")
    
    def save_results_to_file(self, word_frequencies: Dict[str, int], 
                           filename: str = "word_frequencies.txt") -> None:
        """
        –ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —É —Ñ–∞–π–ª
        
        Args:
            word_frequencies: –°–ª–æ–≤–Ω–∏–∫ –∑ —á–∞—Å—Ç–æ—Ç–æ—é —Å–ª—ñ–≤
            filename: –Ü–º'—è —Ñ–∞–π–ª—É –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
        """
        logger.info(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É —Ñ–∞–π–ª: {filename}")
        
        try:
            sorted_words = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"–ê–Ω–∞–ª—ñ–∑ —á–∞—Å—Ç–æ—Ç–∏ —Å–ª—ñ–≤\n")
                f.write(f"{'='*50}\n")
                f.write(f"–í—Å—å–æ–≥–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å–ª—ñ–≤: {len(word_frequencies)}\n")
                f.write(f"–í—Å—å–æ–≥–æ —Å–ª–æ–≤–æ–≤–∂–∏–≤–∞–Ω—å: {sum(word_frequencies.values())}\n\n")
                
                for i, (word, count) in enumerate(sorted_words, 1):
                    f.write(f"{i:4d}. {word:20s} - {count:5d} —Ä–∞–∑—ñ–≤\n")
            
            logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {filename}")
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É: {e}")

def get_sample_urls() -> List[Tuple[str, str]]:
    """–ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –∑—Ä–∞–∑–∫–æ–≤–∏—Ö URL –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è"""
    return [
        ("–ì—É—Ç–µ–Ω–±–µ—Ä–≥ - –ê–ª–∏—Å–∞ –≤ –∫—Ä–∞—ó–Ω—ñ —á—É–¥–µ—Å", 
         "https://www.gutenberg.org/files/11/11-0.txt"),
        ("–ì—É—Ç–µ–Ω–±–µ—Ä–≥ - –®–µ—Ä–ª–æ–∫ –•–æ–ª–º—Å", 
         "https://www.gutenberg.org/files/1661/1661-0.txt"),
        ("–ì—É—Ç–µ–Ω–±–µ—Ä–≥ - –î—Ä–∞–∫—É–ª–∞", 
         "https://www.gutenberg.org/files/345/345-0.txt"),
        ("Wikipedia - Python", 
         "https://en.wikipedia.org/wiki/Python_(programming_language)"),
    ]

def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –ø—Ä–æ–≥—Ä–∞–º–∏"""
    parser = argparse.ArgumentParser(
        description="–ê–Ω–∞–ª—ñ–∑ —á–∞—Å—Ç–æ—Ç–∏ —Å–ª—ñ–≤ —É —Ç–µ–∫—Å—Ç—ñ –∑ URL –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é MapReduce",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–∫–ª–∞–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
  python mapreduce_word_analysis.py --url "https://www.gutenberg.org/files/11/11-0.txt"
  python mapreduce_word_analysis.py --sample-urls
  python mapreduce_word_analysis.py --url "URL" --top-words 15 --workers 8
        """
    )
    
    parser.add_argument(
        '--url',
        help='URL –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É'
    )
    
    parser.add_argument(
        '--sample-urls',
        action='store_true',
        help='–ü–æ–∫–∞–∑–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ –∑—Ä–∞–∑–∫–æ–≤–∏—Ö URL –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è'
    )
    
    parser.add_argument(
        '--top-words',
        type=int,
        default=10,
        help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–ø-—Å–ª—ñ–≤ –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: 10)'
    )
    
    parser.add_argument(
        '--workers',
        type=int,
        default=4,
        help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Ç–æ–∫—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: 4)'
    )
    
    parser.add_argument(
        '--save-results',
        action='store_true',
        help='–ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —É —Ñ–∞–π–ª'
    )
    
    parser.add_argument(
        '--no-visualization',
        action='store_true',
        help='–ù–µ –ø–æ–∫–∞–∑—É–≤–∞—Ç–∏ –≥—Ä–∞—Ñ—ñ–∫'
    )
    
    args = parser.parse_args()
    
    # –ü–æ–∫–∞–∑—É—î–º–æ –∑—Ä–∞–∑–∫–æ–≤—ñ URL —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
    if args.sample_urls:
        print("üìã –ó—Ä–∞–∑–∫–æ–≤—ñ URL –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è:")
        print("=" * 50)
        for name, url in get_sample_urls():
            print(f"‚Ä¢ {name}")
            print(f"  {url}")
            print()
        return
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –≤–∫–∞–∑–∞–Ω–æ URL
    if not args.url:
        print("‚ùå –ü–æ–º–∏–ª–∫–∞: –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ –≤–∫–∞–∑–∞—Ç–∏ URL –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ --sample-urls")
        parser.print_help()
        return
    
    try:
        logger.info("üöÄ –ü–æ—á–∞—Ç–æ–∫ –∞–Ω–∞–ª—ñ–∑—É —á–∞—Å—Ç–æ—Ç–∏ —Å–ª—ñ–≤")
        logger.info(f"üîó URL: {args.url}")
        logger.info(f"üë• –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Ç–æ–∫—ñ–≤: {args.workers}")
        logger.info(f"üèÜ –¢–æ–ø —Å–ª—ñ–≤: {args.top_words}")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä
        analyzer = MapReduceWordAnalyzer(num_workers=args.workers)
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ç–µ–∫—Å—Ç
        text = analyzer.fetch_text_from_url(args.url)
        
        if not text.strip():
            logger.error("‚ùå –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π —Ç–µ–∫—Å—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π")
            return
        
        # –í–∏–∫–æ–Ω—É—î–º–æ MapReduce –∞–Ω–∞–ª—ñ–∑
        word_frequencies = analyzer.mapreduce_word_count(text)
        
        if not word_frequencies:
            logger.error("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")
            return
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–æ–ø-—Å–ª–æ–≤–∞
        top_words = analyzer.get_top_words(word_frequencies, args.top_words)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if args.save_results:
            analyzer.save_results_to_file(word_frequencies)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—é —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if not args.no_visualization:
            analyzer.visualize_top_words(
                top_words, 
                f"–¢–æ–ø-{args.top_words} —Å–ª—ñ–≤ –∑–∞ —á–∞—Å—Ç–æ—Ç–æ—é –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è"
            )
        
        # –í–∏–≤–æ–¥–∏–º–æ –ø—ñ–¥—Å—É–º–∫–æ–≤—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_words = sum(word_frequencies.values())
        unique_words = len(word_frequencies)
        
        print("\n" + "="*60)
        print("üìä –ü–Ü–î–°–£–ú–ö–û–í–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("="*60)
        print(f"üîó URL: {args.url}")
        print(f"üìù –í—Å—å–æ–≥–æ —Å–ª–æ–≤–æ–≤–∂–∏–≤–∞–Ω—å: {total_words:,}")
        print(f"üÜî –£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å–ª—ñ–≤: {unique_words:,}")
        print(f"üìà –°–µ—Ä–µ–¥–Ω—è —á–∞—Å—Ç–æ—Ç–∞: {total_words/unique_words:.1f}")
        print(f"üèÜ –ù–∞–π—á–∞—Å—Ç—ñ—à–µ —Å–ª–æ–≤–æ: '{top_words[0][0]}' ({top_words[0][1]} —Ä–∞–∑—ñ–≤)")
        
        logger.info("‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
        
    except KeyboardInterrupt:
        logger.info("‚ùå –û–ø–µ—Ä–∞—Ü—ñ—é –ø–µ—Ä–µ—Ä–≤–∞–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º")
    except Exception as e:
        logger.error(f"üí• –ü–æ–º–∏–ª–∫–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –ø—Ä–æ–≥—Ä–∞–º–∏: {e}")

if __name__ == "__main__":
    main()
